{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrita\\AppData\\Local\\Temp\\ipykernel_29072\\759698337.py:47: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  _, sr = librosa.load(file_path, sr=None)\n",
      "c:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "C:\\Users\\hrita\\AppData\\Local\\Temp\\ipykernel_29072\\759698337.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_path, sr=None)\n",
      "c:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rate: 48000\n",
      "Segment 0 duration: 30.0 seconds - OK\n",
      "Segment 1 duration: 30.0 seconds - OK\n",
      "Segment 2 duration: 30.0 seconds - OK\n",
      "Segment 3 duration: 30.0 seconds - OK\n",
      "Segment 4 duration: 30.0 seconds - OK\n",
      "Segment 5 duration: 30.0 seconds - OK\n",
      "Segment 6 duration: 30.0 seconds - OK\n",
      "The audio file has 2 channel(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrita\\AppData\\Local\\Temp\\ipykernel_29072\\759698337.py:59: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file_path, sr=None, mono=False)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "TEST Segmentation\n",
    "\n",
    "Goal here is to segment a song into 30 second intervals\n",
    "\"\"\"\n",
    "\n",
    "exp_sr = 3200\n",
    "\n",
    "def segment_song(file_path, segment_length=30, start_offset=10):\n",
    "    \"\"\"\n",
    "    Segment a song into fixed-length parts.\n",
    "\n",
    "    :param file_path: Path to the audio file.\n",
    "    :param segment_length: Length of each segment in seconds.\n",
    "    :param start_offset: Time in seconds to start segmenting from.\n",
    "    :return: List of audio segments.\n",
    "    \"\"\"\n",
    "    # Load the full audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    sample_rate = sr\n",
    "    print('sample rate:', sr)\n",
    "\n",
    "    # Calculate start and end sample for the segmentation\n",
    "    start_sample = int(start_offset * sr)\n",
    "    end_sample = int(len(audio))\n",
    "\n",
    "    # Segment length in samples\n",
    "    segment_sample_length = segment_length * sr\n",
    "\n",
    "    # Split the audio into segments\n",
    "    segments = []\n",
    "    for start in range(start_sample, end_sample, segment_sample_length):\n",
    "        end = start + segment_sample_length\n",
    "        # Check if the segment is shorter than the desired length, discard if necessary\n",
    "        if end <= end_sample:\n",
    "            segment = audio[start:end]\n",
    "            segments.append(segment)\n",
    "        else:\n",
    "            # Optional: Handle the last segment if it's shorter than the desired length\n",
    "            # For example, you can discard it or pad it\n",
    "            pass\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data\\\\A Bigger Fear - A GlitchTale Soundtrack (Commission).m4a'\n",
    "_, sr = librosa.load(file_path, sr=None)\n",
    "segments = segment_song(file_path)\n",
    "\n",
    "expected_duration = 30  # in seconds\n",
    "for i, segment in enumerate(segments):\n",
    "    duration = len(segment) / sr  # 'sr' is the sample rate\n",
    "    if duration >= expected_duration:\n",
    "        print(f\"Segment {i} duration: {duration} seconds - OK\")\n",
    "    else:\n",
    "        print(f\"Segment {i} duration: {duration} seconds - Too Short\")\n",
    "        \n",
    "# Find number of channels\n",
    "audio, _ = librosa.load(file_path, sr=None, mono=False)\n",
    "num_channels = audio.shape[0] if audio.ndim > 1 else 1\n",
    "\n",
    "print(f\"The audio file has {num_channels} channel(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrita\\AppData\\Local\\Temp\\ipykernel_28252\\1461942464.py:8: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file_path, sr=sr)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "DO Segmentation\n",
    "\"\"\"\n",
    "\n",
    "def segment_audio(file_path, segment_length=30, start_offset=10, sr=exp_sr):\n",
    "    # Load and segment the audio file as before\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    segments = []\n",
    "    start_sample = int(start_offset * sr)\n",
    "    segment_samples = segment_length * sr\n",
    "\n",
    "    for start in range(start_sample, len(audio), segment_samples):\n",
    "        end = start + segment_samples\n",
    "        if end <= len(audio):\n",
    "            segments.append(audio[start:end])\n",
    "    return segments\n",
    "\n",
    "def process_directory(directory_path, output_directory=None):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.m4a'):  # Check for m4a files\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            segments = segment_audio(file_path)\n",
    "\n",
    "            if output_directory:\n",
    "                save_segments(segments, filename, output_directory)\n",
    "\n",
    "def save_segments(segments, original_filename, output_directory, sr=exp_sr):\n",
    "    base_filename = os.path.splitext(original_filename)[0]\n",
    "    for i, segment in enumerate(segments):\n",
    "        output_filename = f\"{base_filename}_segment_{i}.wav\"  # Change format if needed\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "        sf.write(output_path, segment, sr)\n",
    "\n",
    "# Example usage\n",
    "process_directory('C:\\\\Hrita\\\\Code\\\\music_generator\\\\data', 'C:\\\\Hrita\\\\Code\\\\music_generator\\\\data_segments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet1DModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "# os imported\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio  # For audio processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Train-Test split\n",
    "\"\"\"\n",
    "\n",
    "data_dir = 'data_segments/'\n",
    "\n",
    "files = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_files, test_files = train_test_split(files, test_size=0.2)  # 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Dataset\n",
    "\"\"\"\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.file_paths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        \n",
    "        return waveform, sample_rate\n",
    "    \n",
    "# Create train and test datasets\n",
    "train_dataset = AudioDataset(train_files)\n",
    "test_dataset = AudioDataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Dataloaders\n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet1DModel(\n",
       "  (time_proj): GaussianFourierProjection()\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock1D(\n",
       "      (down): Downsample1d()\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1-2): 2 x ResConvBlock(\n",
       "          (conv_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnDownBlock1D(\n",
       "      (down): Downsample1d()\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x SelfAttention1d(\n",
       "          (group_norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1-2): 2 x ResConvBlock(\n",
       "          (conv_1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DownBlock1D(\n",
       "      (down): Downsample1d()\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1-2): 2 x ResConvBlock(\n",
       "          (conv_1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock1D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1): ResConvBlock(\n",
       "          (conv_1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (2): ResConvBlock(\n",
       "          (conv_skip): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (up): Upsample1d()\n",
       "    )\n",
       "    (1): AttnUpBlock1D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x SelfAttention1d(\n",
       "          (group_norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (proj_attn): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(128, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1-2): 2 x ResConvBlock(\n",
       "          (conv_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (up): Upsample1d()\n",
       "    )\n",
       "    (2): UpBlock1D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResConvBlock(\n",
       "          (conv_skip): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (1): ResConvBlock(\n",
       "          (conv_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "        (2): ResConvBlock(\n",
       "          (conv_skip): Conv1d(32, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (conv_1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (gelu_1): GELU(approximate='none')\n",
       "          (conv_2): Conv1d(32, 1, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (group_norm_2): GroupNorm(1, 1, eps=1e-05, affine=True)\n",
       "          (gelu_2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (up): Upsample1d()\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock1D(\n",
       "    (down): Downsample1d()\n",
       "    (up): Upsample1d()\n",
       "    (attentions): ModuleList(\n",
       "      (0-5): 6 x SelfAttention1d(\n",
       "        (group_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_attn): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-5): 6 x ResConvBlock(\n",
       "        (conv_1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (group_norm_1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (gelu_1): GELU(approximate='none')\n",
       "        (conv_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (group_norm_2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (gelu_2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model: class diffusers.UNet1DModel\n",
    "\"\"\"\n",
    "\n",
    "sample_rate = exp_sr\n",
    "duration_of_segment = 30    # seconds\n",
    "sample_size = sample_rate * duration_of_segment\n",
    "\n",
    "model = UNet1DModel(\n",
    "    sample_size=sample_size,        # Size of your input samples\n",
    "    in_channels=1,           # Number of input channels, e.g., 1 for mono audio\n",
    "    out_channels=1,          # Number of output channels\n",
    "    # layers_per_block=(2, 2, 2, 2),  # Layers in each downsampling/upsampling block\n",
    "    down_block_types=(\"DownBlock1D\", \"AttnDownBlock1D\", \"DownBlock1D\"),\n",
    "    up_block_types=(\"UpBlock1D\", \"AttnUpBlock1D\", \"UpBlock1D\"),\n",
    "    block_out_channels=[32, 64, 128]\n",
    "    # other configuration parameters if needed\n",
    ")\n",
    "\n",
    "# optimizer and loss funciton\n",
    "device = 'cuda'\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 96000])\n",
      "torch.Size([1, 1, 96000])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(noisy_inputs, torch\u001b[38;5;241m.\u001b[39mtensor([timestep], device\u001b[38;5;241m=\u001b[39mdevice), return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\torch\\nn\\functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   3316\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[0;32m   3317\u001b[0m     )\n\u001b[1;32m-> 3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[0;32m   3319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3323\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   3324\u001b[0m     )\n\u001b[0;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Training loop\n",
    "\"\"\"\n",
    "def add_noise(data, noise_level):\n",
    "    return data + noise_level * torch.randn_like(data)\n",
    "\n",
    "def noise_schedule(timestep, num_timesteps):\n",
    "    # Example of a linear noise schedule\n",
    "    return timestep / num_timesteps\n",
    "\n",
    "num_timesteps = 1000 \n",
    "num_epochs = 10  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        # In a diffusion model, we typically select a random timestep for each batch\n",
    "        timestep = torch.randint(0, num_timesteps, (1,), device=device).item()\n",
    "        noise_level = noise_schedule(timestep, num_timesteps)\n",
    "        noisy_inputs = add_noise(inputs, noise_level).to(device)\n",
    "        print(noisy_inputs.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(noisy_inputs, torch.tensor([timestep], device=device), return_dict = False)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
